{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "y, x = load_data(sub_sample=False, add_outlier=False, train=True)\n",
    "y_te, x_te = load_data(sub_sample=False, add_outlier=False, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x, mean_x, std_x = standardize(x)\n",
    "y, tx = build_model_data(x, y)\n",
    "x_te, mean_xte, std_xte = standardize(x_te)\n",
    "y_te, tx_te = build_model_data(x_te, y_te)\n",
    "#tx,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the Cost Function\n",
    "Fill in the `compute_cost` function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mse(e):\n",
    "    \"\"\"Calculate the mse for vector e.\"\"\"\n",
    "    return 1/2*np.mean(e**2)\n",
    "\n",
    "\n",
    "def calculate_mae(e):\n",
    "    \"\"\"Calculate the mae for vector e.\"\"\"\n",
    "    return np.mean(np.abs(e))\n",
    "\n",
    "\n",
    "def compute_loss(y, tx, w):\n",
    "    \"\"\"Calculate the loss.\n",
    "\n",
    "    You can calculate the loss using mse or mae.\n",
    "    \"\"\"\n",
    "    e = y - tx.dot(w)\n",
    "    return calculate_mse(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Compute the gradient.\"\"\"\n",
    "    err = y - tx.dot(w)\n",
    "    grad = -tx.T.dot(err) / len(err)\n",
    "    return grad, err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"Gradient descent algorithm.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    #losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # compute loss, gradient\n",
    "        grad, err = compute_gradient(y, tx, w)\n",
    "        loss = calculate_mse(err)\n",
    "        # gradient w by descent update\n",
    "        w = w - gamma * grad\n",
    "        # store w and loss\n",
    "        #losses.append(loss)\n",
    "        #print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              #bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 150\n",
    "gamma = 0.1\n",
    "\n",
    "# Initialization\n",
    "w_initial = 0.1*np.ones(tx.shape[1])\n",
    "\n",
    "# Start gradient descent.\n",
    "#start_time = datetime.datetime.now()\n",
    "gradient_loss, gradient_w = gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "gradient_loss_te = compute_loss(y_te, tx_te, gradient_w)\n",
    "#end_time = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.08530420147924032, 0.24137040482184374)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_loss,gradient_loss_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.34266797,  0.01726452, -0.12133663, -0.10846244,  0.00162979,\n",
       "       -0.00858062,  0.1621876 , -0.01210013,  0.11996346, -0.00532339,\n",
       "       -0.01345443, -0.06396064,  0.05820906, -0.00949754,  0.10634043,\n",
       "       -0.00042367, -0.000675  ,  0.10992295, -0.0004416 ,  0.00124521,\n",
       "        0.05075837,  0.00052764, -0.03722702, -0.07092928,  0.01653835,\n",
       "        0.02079013,  0.02104429, -0.01036036, -0.00984098, -0.00982033,\n",
       "       -0.0376149 ])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stoch_gradient(y, tx, w):\n",
    "    \"\"\"Compute a stochastic gradient from just few examples n and their corresponding y_n labels.\"\"\"\n",
    "    err = y - tx.dot(w)\n",
    "    grad = -tx.T.dot(err) / len(err)\n",
    "    return grad, err\n",
    "\n",
    "def stochastic_gradient_descent(\n",
    "        y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"Stochastic gradient descent.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    #ws = [initial_w]\n",
    "    #losses = []\n",
    "    w = initial_w\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "        for y_batch, tx_batch in batch_iter(y, tx, batch_size=batch_size, num_batches=1):\n",
    "            # compute a stochastic gradient and loss\n",
    "            grad, _ = compute_stoch_gradient(y_batch, tx_batch, w)\n",
    "            # update w through the stochastic gradient update\n",
    "            w = w - gamma * grad\n",
    "            # calculate loss\n",
    "            loss = compute_loss(y, tx, w)\n",
    "            # store w and loss\n",
    "            #ws.append(w)\n",
    "            #losses.append(loss)\n",
    "\n",
    "        #print(\"SGD({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              #bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    return loss, w\n",
    "# from stochastic_gradient_descent import *\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.7\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = 0.1*np.ones(tx.shape[1])\n",
    "\n",
    "# Start SGD.\n",
    "#start_time = datetime.datetime.now()\n",
    "sgd_loss, sgd_w = stochastic_gradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma)\n",
    "#end_time = datetime.datetime.now()\n",
    "sgd_loss_te = compute_loss(y_te, tx_te, sgd_w)\n",
    "# Print result\n",
    "#exection_time = (end_time - start_time).total_seconds()\n",
    "#print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.1671159417604567e+65, 1.166952206493414e+65)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_loss,sgd_loss_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(tx),print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares(y, tx):\n",
    "    \"\"\"calculate the least squares solution.\"\"\"\n",
    "    a = tx.T.dot(tx)\n",
    "    b = tx.T.dot(y)\n",
    "    w = np.linalg.solve(a, b)\n",
    "    loss = compute_loss(y, tx, w)\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_loss, ls_w = least_squares(y, tx)\n",
    "ls_loss_te = compute_loss(y_te, tx_te, ls_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.08486139962233243, 0.2655687176380621)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls_loss,ls_loss_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lambda_):\n",
    "    \"\"\"implement ridge regression.\"\"\"\n",
    "    aI = 2 * tx.shape[0] * lambda_ * np.identity(tx.shape[1])\n",
    "    a = tx.T.dot(tx) + aI\n",
    "    b = tx.T.dot(y)\n",
    "    w = np.linalg.solve(a, b)\n",
    "    loss = compute_loss(y, tx, w)\n",
    "    return loss, w\n",
    "# define parameter\n",
    "lambdas = np.logspace(-5, 0, 15)\n",
    "rdrg_losses = []\n",
    "rdrg_losses_te = []\n",
    "rdrg_ws = []\n",
    "for ind, lambda_ in enumerate(lambdas):\n",
    "    # ridge regression\n",
    "    rdrg_loss, rdrg_w = ridge_regression(y, tx, lambda_)\n",
    "    rdrg_loss_te = compute_loss(y_te, tx_te, rdrg_w)\n",
    "    rdrg_losses.append(rdrg_loss)\n",
    "    rdrg_losses_te.append(rdrg_loss_te)\n",
    "    rdrg_ws.append(rdrg_w)\n",
    "    #rmse_tr.append(np.sqrt(2 * compute_mse(y_tr, tx_tr, weight)))\n",
    "    #rmse_te.append(np.sqrt(2 * compute_mse(y_te, tx_te, weight)))\n",
    "    #print(\"proportion={p}, degree={d}, lambda={l:.3f}, Training RMSE={tr:.3f}, Testing RMSE={te:.3f}\".format(\n",
    "           #p=ratio, d=degree, l=lambda_, tr=rmse_tr[ind], te=rmse_te[ind]))\n",
    "#plot_train_test(rmse_tr, rmse_te, lambdas, degree)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.08495553210784819,\n",
       "  0.08497782126202305,\n",
       "  0.0849911773447907,\n",
       "  0.08499834465694674,\n",
       "  0.0850022497169442,\n",
       "  0.085005086622608,\n",
       "  0.08501031826692962,\n",
       "  0.08503084462381698,\n",
       "  0.08512011733444838,\n",
       "  0.08546299477484444,\n",
       "  0.08651924717608842,\n",
       "  0.08910479140635215,\n",
       "  0.0947294801425132,\n",
       "  0.10587091734858638,\n",
       "  0.12276418631822966],\n",
       " [0.24361002759146122,\n",
       "  0.24360699679019235,\n",
       "  0.24360928736143997,\n",
       "  0.24361775147430037,\n",
       "  0.24363870660481282,\n",
       "  0.2436884952881576,\n",
       "  0.2438082163834528,\n",
       "  0.24410839297670336,\n",
       "  0.2449102141909508,\n",
       "  0.2471583133215684,\n",
       "  0.2533032898883575,\n",
       "  0.2681770961664956,\n",
       "  0.2979910117427005,\n",
       "  0.3448074328625563,\n",
       "  0.39882084106483123])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdrg_losses,rdrg_losses_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
